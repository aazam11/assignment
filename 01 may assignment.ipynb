{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c3edb0-987e-4e8b-abd7-e2bfb6dbdd5f",
   "metadata": {},
   "source": [
    "#Q1\n",
    "A contingency matrix, also known as a confusion matrix, is a table that describes the performance of a classification model. It compares the predicted class labels with the true class labels and categorizes them into four groups: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). It is a useful tool for evaluating the performance of a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a23c9c3-e6f2-4f60-87e7-c11f9dc0f8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3 2]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Example data\n",
    "true_labels = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "predicted_labels = [1, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d9d08-ff27-41ad-bfb0-824a93a8594d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1666eb59-11cd-4a0a-8ef9-8d2c3733ea0c",
   "metadata": {},
   "source": [
    "#Q2\n",
    "A pair confusion matrix is an extension of the regular confusion matrix, specifically designed for binary and multiclass classification problems. It considers pairs of classes and provides information about the pairwise classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0501b8e1-c66d-4e79-bfde-aa894c87af9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair Confusion Matrix:\n",
      "[[2 3]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Example data\n",
    "true_labels = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "predicted_labels = [1, 0, 1, 0, 1, 1, 1, 0, 1, 1]\n",
    "\n",
    "# Create a pair confusion matrix\n",
    "pair_cm = confusion_matrix(true_labels, predicted_labels, labels=[0, 1])\n",
    "\n",
    "# Print the pair confusion matrix\n",
    "print(\"Pair Confusion Matrix:\")\n",
    "print(pair_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888bddb-d9dc-443d-bb0c-81928e606bce",
   "metadata": {},
   "source": [
    "In this example, the pair confusion matrix is the same as the regular confusion matrix for a binary classification problem. However, in a multiclass scenario, the pair confusion matrix would provide information about each pair of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdcf47f-ff8f-45e3-8aed-f26cf01e88bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e70c742-f4f2-4444-9e3c-bb4168003d5b",
   "metadata": {},
   "source": [
    "#Q3\n",
    "Extrinsic measures in natural language processing (NLP) evaluate the performance of a language model in the context of a downstream task, such as sentiment analysis or named entity recognition. Instead of assessing the model in isolation, extrinsic measures focus on its effectiveness in solving real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "535efb62-b676-48a5-bc96-4c36499802df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Load the 20 newsgroups dataset (as a sample dataset)\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Increase max_iter\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create a pipeline with scaling and logistic regression\n",
    "classifier = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate accuracy as an extrinsic measure\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de14409-7758-4595-ba6c-80b7e8e4c4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f5786c2-df5e-4349-9119-b0926b2a4447",
   "metadata": {},
   "source": [
    "#Q4\n",
    "Intrinsic measures in machine learning focus on evaluating a model's performance based on its internal characteristics, such as precision, recall, F1 score, etc., without considering a specific downstream task. Unlike extrinsic measures, intrinsic measures assess the model's capabilities in a more generic sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b4448a-f1d2-4a2e-982d-5ba40f3a38d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.97\n",
      "Recall: 0.97\n",
      "F1 Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_digits\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Load the digits dataset (as a sample dataset)\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate precision, recall, and F1 score as intrinsic measures\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a16a6-645c-4589-80b2-7a5154bfa37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f83611c-c9dc-4fec-b3ad-f4333e6be554",
   "metadata": {},
   "source": [
    "#Q5\n",
    "The confusion matrix is a key tool in machine learning for evaluating the performance of a classification model. It provides a detailed breakdown of correct and incorrect predictions, allowing the identification of strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e233d1-e968-4502-b816-3eb3721bed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset (as a sample dataset)\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e5bba-b933-4eaa-ac35-cf910815c6af",
   "metadata": {},
   "source": [
    "the confusion matrix shows the classifier's performance on the Iris dataset, where each row represents the true class, and each column represents the predicted class. By analyzing the confusion matrix, you can identify strengths (correct predictions on the diagonal) and weaknesses (off-diagonal elements indicating misclassifications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e1ecb-e8d7-4886-9c74-86f0ad08e512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e64a081-ed97-4000-a5ce-54fc8f15d25c",
   "metadata": {},
   "source": [
    "#Q6\n",
    "Common intrinsic measures for unsupervised learning include silhouette score, Davies-Bouldin index, and the Calinski-Harabasz index. These measures assess the quality and structure of clusters generated by unsupervised algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e59571-b17d-4619-a2fc-85a892b8dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.85\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data with three clusters\n",
    "X, y = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculate silhouette score as an intrinsic measure\n",
    "silhouette_avg = silhouette_score(X, y_kmeans)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e327ed25-a885-44df-b069-d41536432b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74a36523-4ba3-48e6-9789-2b47905d6a71",
   "metadata": {},
   "source": [
    "#Q7\n",
    "Accuracy can be misleading in certain situations, especially when dealing with imbalanced datasets. Consider a binary classification problem where 95% of the samples belong to class A and 5% to class B. If a model predicts all instances as class A, it would achieve 95% accuracy, even though it fails to identify any instances of class B.\n",
    "\n",
    "To address this, other metrics such as precision, recall, F1 score, and the area under the ROC curve (AUC-ROC) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d7ccfa1-7446-4d13-9654-066c9099863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n",
      "Precision: 0.97\n",
      "Recall: 0.97\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the digits dataset (as a sample dataset)\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy, precision, and recall\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78728e7e-6af2-45b1-abcc-acb414434ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
