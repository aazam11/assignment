{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f63cbef7-658d-431b-b5da-3bfe5f47db6a",
   "metadata": {},
   "source": [
    "#Q1. How does bagging reduce overfitting in decision trees?\n",
    "\n",
    "Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by training multiple independent models on different subsets of the training data and then combining their predictions. This helps to reduce the variance of the model, making it more robust and less prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221ddc6e-ed1d-4816-a499-17ec24d8f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier as the base learner\n",
    "base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a bagging classifier with 100 base learners\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the bagging classifier\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08108537-c767-42a2-8660-3063df3948ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "214f587a-6f1b-449c-a20d-f5b75590b0ff",
   "metadata": {},
   "source": [
    "#Q2 What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Diversity: Using different types of base learners can introduce diversity in the ensemble, making it more robust and adaptable to different types of data patterns.\n",
    "\n",
    "Improved Generalization: Combining the predictions of diverse base learners can lead to better generalization performance on unseen data.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Computational Complexity: Training and combining predictions from diverse base learners can be computationally expensive, especially if the base learners are complex models.\n",
    "\n",
    "Increased Variance: If the base learners are too complex and have high variance, it may lead to an increase in the overall variance of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0cf21-f053-4130-b3ed-f8f95e0061dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4f0cd5c-441f-4fda-8095-72cd3b8c5c87",
   "metadata": {},
   "source": [
    "#Q3.How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "\n",
    "The choice of the base learner in bagging affects the bias-variance tradeoff. Typically, using base learners with high variance and low bias (e.g., deep decision trees) benefits more from bagging, as it helps reduce the variance. However, if the base learner already has low variance, the improvement may be less noticeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a39cc06-1515-4eb4-bd12-4b8f24583dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier with low depth as the base learner\n",
    "low_depth_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Create a bagging classifier with 100 base learners\n",
    "bagging_low_depth_tree = BaggingClassifier(low_depth_tree, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the bagging classifier\n",
    "bagging_low_depth_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_low_depth_tree.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47c1a2-1894-46cd-aa39-f7aa9bebecea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abec63f4-7d8d-44f6-8754-4c226549929c",
   "metadata": {},
   "source": [
    "#Q4 Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "\n",
    "Yes, bagging can be used for both classification and regression tasks. In classification, it involves aggregating the predictions of individual models to make a final decision, while in regression, the predictions are typically averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8136bd84-4a24-4e10-8134-5cbeea3abdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2970.863235955056\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree regressor as the base learner\n",
    "base_regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Create a bagging regressor with 100 base learners\n",
    "bagging_regressor = BaggingRegressor(base_regressor, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the bagging regressor\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b61a7-7b8c-47a1-b57e-b795fec7d136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a62f2da-5e86-4bf3-bc49-001ca31702fd",
   "metadata": {},
   "source": [
    "#Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "\n",
    "The ensemble size in bagging refers to the number of base learners (models) that are trained on different subsets of the training data. Generally, increasing the ensemble size can improve the performance up to a certain point, after which the benefits may diminish or even lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090cc869-2b50-4398-bbf0-b5823fb000f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Size: 10, Accuracy: 1.0\n",
      "Ensemble Size: 20, Accuracy: 1.0\n",
      "Ensemble Size: 30, Accuracy: 1.0\n",
      "Ensemble Size: 40, Accuracy: 1.0\n",
      "Ensemble Size: 50, Accuracy: 1.0\n",
      "Ensemble Size: 60, Accuracy: 1.0\n",
      "Ensemble Size: 70, Accuracy: 1.0\n",
      "Ensemble Size: 80, Accuracy: 1.0\n",
      "Ensemble Size: 90, Accuracy: 1.0\n",
      "Ensemble Size: 100, Accuracy: 1.0\n",
      "Ensemble Size: 110, Accuracy: 1.0\n",
      "Ensemble Size: 120, Accuracy: 1.0\n",
      "Ensemble Size: 130, Accuracy: 1.0\n",
      "Ensemble Size: 140, Accuracy: 1.0\n",
      "Ensemble Size: 150, Accuracy: 1.0\n",
      "Ensemble Size: 160, Accuracy: 1.0\n",
      "Ensemble Size: 170, Accuracy: 1.0\n",
      "Ensemble Size: 180, Accuracy: 1.0\n",
      "Ensemble Size: 190, Accuracy: 1.0\n",
      "Ensemble Size: 200, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier as the base learner\n",
    "base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Vary the ensemble size from 10 to 200 in increments of 10\n",
    "ensemble_sizes = range(10, 210, 10)\n",
    "\n",
    "for size in ensemble_sizes:\n",
    "    # Create a bagging classifier with the current ensemble size\n",
    "    bagging_classifier = BaggingClassifier(base_classifier, n_estimators=size, random_state=42)\n",
    "\n",
    "    # Train the bagging classifier\n",
    "    bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = bagging_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Ensemble Size: {size}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934fcf4-0fb7-4e9a-a746-37dbd1a63817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b20ebd88-1ffe-4a55-b444-408ec6054158",
   "metadata": {},
   "source": [
    "#Q6. Can you provide an example of a real-world application of bagging in machine learning?\n",
    "\n",
    "One real-world application of bagging is in the field of bioinformatics, particularly in the prediction of protein-protein interactions (PPIs). Predicting PPIs is crucial for understanding cellular processes, disease mechanisms, and drug discovery.\n",
    "\n",
    "In this context, machine learning models, often based on decision trees, are employed to predict whether a pair of proteins will interact. Due to the complexity and noise in biological data, using a single model may result in overfitting. Bagging techniques, such as Random Forests (an ensemble of decision trees), can enhance the prediction accuracy by aggregating the predictions of multiple models trained on different subsets of protein interaction data.\n",
    "\n",
    "This approach helps in dealing with the inherent variability in biological data, improves the robustness of the predictions, and provides more reliable insights into the complex network of protein interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
