{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed56cfc-5017-4b39-8a4b-ea7a0d0d6269",
   "metadata": {},
   "source": [
    "Q1\n",
    "Elastic Net Regression is a linear regression technique that combines both L1 (Lasso) and L2 (Ridge) regularization terms in its objective function. The regularization term in Elastic Net is a linear combination of the L1 and L2 penalties. This combination allows Elastic Net to address some limitations of Lasso and Ridge regression, providing a balance between feature selection and handling correlated features.\n",
    "\n",
    "Elastic Net objective function:\n",
    "Cost=OLS Cost+λ1 ∑i=1 ∣wi∣+λ2 ∑ i=1 wi^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef46137-832e-4edc-9302-7fa55d4bb4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad0b684-d3d1-4849-829b-c8b9bc697d30",
   "metadata": {},
   "source": [
    "Q2\n",
    "The optimal values of the regularization parameters (1λ and λ2) in Elastic Net are typically chosen using cross-validation. Grid search or randomized search over a range of values for both parameters can be performed, and the combination with the best performance is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e0b51b-56c9-4fb7-846f-712d7ffa4e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 0.1\n",
      "Optimal l1_ratio: 0.1\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create Elastic Net model\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Define a range of alpha and l1_ratio values to test\n",
    "param_grid = {'alpha': [0.1, 1.0, 10.0],\n",
    "              'l1_ratio': [0.1, 0.5, 0.7, 0.9]}\n",
    "\n",
    "# Perform GridSearchCV to find the optimal alpha and l1_ratio\n",
    "grid_search = GridSearchCV(elastic_net, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best alpha and l1_ratio from the grid search\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "\n",
    "print(f\"Optimal alpha: {best_alpha}\")\n",
    "print(f\"Optimal l1_ratio: {best_l1_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee807e-2d49-4e6d-8f73-f61293ceeb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0364b773-6ead-46a9-9021-476017eefb9b",
   "metadata": {},
   "source": [
    "Q3. What are the advantages and disadvantages of Elastic Net Regression?\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Balances L1 and L2 regularization: Combines the feature selection ability of Lasso with the ability of Ridge to handle correlated features.\n",
    "Handles multicollinearity: Effective when dealing with highly correlated features.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Two hyperparameters: Requires tuning of two hyperparameters (1λ and λ2), making the model selection process more complex.\n",
    "Computationally more expensive: More computationally intensive compared to Lasso and Ridge due to the additional term in the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efca7ce-7167-4117-aa19-58e0ff83721e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91f12457-11d3-48a0-b459-751a94ad1b7a",
   "metadata": {},
   "source": [
    "Q4. What are some common use cases for Elastic Net Regression?\n",
    "\n",
    "Elastic Net Regression is commonly used in scenarios where:\n",
    "\n",
    "The dataset has many features, some of which may be irrelevant or redundant.\n",
    "Features are correlated, and you want to prevent multicollinearity issues.\n",
    "A balance between feature selection and regularization is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc83ac8-6c36-4854-87f1-d37826311423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9318a31-d1fe-4e73-b87b-05c1276183a6",
   "metadata": {},
   "source": [
    "Q5 In Elastic Net Regression, the model includes both L1 (Lasso) and L2 (Ridge) regularization terms, which are controlled by hyperparameters denoted as \"alpha\" and \"l1_ratio.\" The coefficients in Elastic Net Regression are influenced by these regularization terms, and their interpretation is somewhat similar to that in linear regression, but with some nuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ff7793-abf4-43e2-b55a-6de34f6de167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 0.7013609376405553\n",
      "Coefficients: [ 0.23073855  0.43243295 -0.61106162  0.         -0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)\n",
    "y = 2 * X[:, 0] + 3 * X[:, 1] - 4 * X[:, 2] + np.random.randn(100)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit Elastic Net model\n",
    "alpha = 0.5  # Regularization strength\n",
    "l1_ratio = 0.5  # Mix ratio between L1 and L2 penalties\n",
    "elastic_net = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Interpret coefficients\n",
    "coefficients = elastic_net.coef_\n",
    "intercept = elastic_net.intercept_\n",
    "\n",
    "# Print coefficients and intercept\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87e2e9-68dd-4c7c-aa91-f6b7a1da3bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac209e06-9b56-41be-b972-f482a7c4fee4",
   "metadata": {},
   "source": [
    "Q6 Handling missing values is an important step in the data preprocessing phase before applying any machine learning model, including Elastic Net Regression. Here are some common strategies to deal with missing values:\n",
    "\n",
    "Imputation:\n",
    "\n",
    "One approach is to fill missing values with a central tendency measure such as the mean, median, or mode of the column. This is known as imputation.\n",
    "Scikit-learn provides the SimpleImputer class for this purpose.\n",
    "\n",
    "Dropping Missing Values:\n",
    "\n",
    "Another option is to remove rows or columns with missing values. However, this should be done cautiously, as it might lead to a significant loss of information.\n",
    "Pandas provides the dropna method to drop missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0508cd3b-3a6f-4685-85b1-1ecbf9ee7588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.4657205148976122\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data with missing values\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)\n",
    "X[X < 0.2] = np.nan  # Introduce missing values\n",
    "y = 2 * X[:, 0] + 3 * X[:, 1] - 4 * X[:, 2] + np.random.randn(100)\n",
    "\n",
    "# Convert to DataFrame for easy handling of missing values\n",
    "df = pd.DataFrame(X, columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'])\n",
    "df['target'] = y\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df.dropna(subset=['target'], inplace=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop('target', axis=1)\n",
    "y_train = train_df['target']\n",
    "\n",
    "X_test = test_df.drop('target', axis=1)\n",
    "y_test = test_df['target']\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Fit Elastic Net model\n",
    "alpha = 0.5\n",
    "l1_ratio = 0.5\n",
    "elastic_net = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = np.mean((elastic_net.predict(X_test_scaled) - y_test) ** 2)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ef55c-bc23-478e-9746-fce7a9d6f322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd0b82aa-8d48-4044-8bb9-04dd702e17b3",
   "metadata": {},
   "source": [
    "Q7Elastic Net inherently performs feature selection by driving some coefficients to exactly zero. The degree of sparsity in the model depends on the strength of the L1 penalty (λ1). A larger λ1 encourages more coefficients to become zero, leading to a sparser model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "880be1fc-1c10-41fe-af27-c22baa40243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.18943499524127297\n",
      "Selected l1_ratio: 0.5\n",
      "Selected coefficients: [ 1.89206516 -9.96604966 24.43510742 15.49023732 -5.82582946 -3.69773061\n",
      " -8.879132    7.06088057 19.34684644  3.56025388]\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Create Elastic Net model with cross-validated alpha and l1_ratio\n",
    "elastic_net_cv = ElasticNetCV(cv=5)\n",
    "elastic_net_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Access the selected alpha and l1_ratio\n",
    "selected_alpha = elastic_net_cv.alpha_\n",
    "selected_l1_ratio = elastic_net_cv.l1_ratio_\n",
    "\n",
    "# Access the coefficients\n",
    "coefficients = elastic_net_cv.coef_\n",
    "\n",
    "print(f\"Selected alpha: {selected_alpha}\")\n",
    "print(f\"Selected l1_ratio: {selected_l1_ratio}\")\n",
    "print(\"Selected coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0eab43-68f5-4b4f-b6e6-e86bb935cb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d789a52-479c-40d1-97ec-1f00830de953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8\n",
    "#Pickle is a Python module for serializing and deserializing objects. You can use it to save and load trained models.\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Train Elastic Net model\n",
    "elastic_net_model = ElasticNet(alpha=best_alpha, l1_ratio=best_l1_ratio)\n",
    "elastic_net_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the model to a file\n",
    "with open('elastic_net_model.pkl', 'wb') as file:\n",
    "    pickle.dump(elastic_net_model, file)\n",
    "\n",
    "# Load the model from the file\n",
    "with open('elastic_net_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0a5b3-640a-4e97-9ca6-9a83d411a022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce10258f-4472-4298-8ec4-0c89d5245367",
   "metadata": {},
   "source": [
    "Q9. What is the purpose of pickling a model in machine learning?\n",
    "\n",
    "Pickling a model allows you to:\n",
    "\n",
    "Save: Store a trained model to disk for later use or sharing with others.\n",
    "\n",
    "Deploy: Use the model in production environments without retraining.\n",
    "\n",
    "Versioning: Maintain a record of model versions for reproducibility.\n",
    "\n",
    "These practices are essential for ensuring consistent and reproducible machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754f386-7fb3-4b1d-ba06-6819f43ffc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
