{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409af491-a131-4c44-8161-78fa3582672e",
   "metadata": {},
   "source": [
    "#Q1: What is the KNN algorithm?\n",
    "\n",
    "K-Nearest Neighbors (KNN): It is a simple and effective supervised learning algorithm used for classification and regression tasks. The principle behind KNN is to find the K data points in the training set that are closest to a given test data point and make predictions based on the majority class (for classification) or the average (for regression) of these K neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfea2d2-6fc6-4058-a82c-8151fb60c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create KNN classifier with K=3\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8ae4d-5a84-4526-803d-033b6d76b163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "036aafa3-f7be-4bbe-9ead-2b9f4d0c2e5b",
   "metadata": {},
   "source": [
    "#Q2: How do you choose the value of K in KNN?\n",
    "\n",
    "Choosing K in KNN:\n",
    "The value of K has a significant impact on the performance of the KNN algorithm. A smaller K makes the model sensitive to noise, while a larger K may smooth out patterns. It's common to use cross-validation to find the optimal K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8acfaf92-3e37-4ca0-a28f-a468d3068c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal K: 11\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define a range of K values to try\n",
    "k_values = list(range(1, 21))\n",
    "\n",
    "# Perform 10-fold cross-validation for each K\n",
    "cv_scores = []\n",
    "for k in k_values:\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn_classifier, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find the optimal K\n",
    "optimal_k = k_values[cv_scores.index(max(cv_scores))]\n",
    "\n",
    "print(f\"Optimal K: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f3673-a1fe-4176-9286-584ad0fe0e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59ffc62a-a089-4563-b5ed-6131da9f1b12",
   "metadata": {},
   "source": [
    "#Q3: What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Difference between KNN Classifier and Regressor:\n",
    "\n",
    "KNN Classifier: Used for classification tasks, where the output is a class label.\n",
    "KNN Regressor: Used for regression tasks, where the output is a continuous value.\n",
    "\n",
    "In scikit-learn, the only difference is in the class you import:\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier  # for classification\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor  # for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f8b61-1825-45ab-b687-8fb61b872f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa347773-f756-43bf-a1d4-c70f278f77e4",
   "metadata": {},
   "source": [
    "#Q4 Q4: How do you measure the performance of KNN? \n",
    "\n",
    "Measuring KNN Performance:\n",
    "Common metrics for classification include accuracy, precision, recall, F1 score, and for regression, metrics like Mean Squared Error (MSE) or R-squared can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd74332-29a6-4554-81b9-3871cb6fdef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create KNN classifier with K=3\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Assuming you have a trained KNN classifier (knn_classifier)\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829f2cd-37ec-40cd-ba39-0ae8391b45a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a57b041-cfef-471c-a095-431eacac308b",
   "metadata": {},
   "source": [
    "#Q5: What is the curse of dimensionality in KNN?\n",
    "\n",
    "Curse of Dimensionality in KNN:\n",
    "As the number of features (dimensions) increases, the distance between data points increases, making the notion of proximity less meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5619559f-0a8f-48d5-8476-ae4ffed6d714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5\n",
    "# Create a synthetic dataset with varying dimensions\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X_high_dimension, y_high_dimension = make_classification(n_samples=100, n_features=1000, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_high_dimension, X_test_high_dimension, y_train_high_dimension, y_test_high_dimension = train_test_split(\n",
    "    X_high_dimension, y_high_dimension, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Use KNN on the high-dimensional dataset\n",
    "knn_classifier_high_dimension = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier_high_dimension.fit(X_train_high_dimension, y_train_high_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947184f-3832-427b-99dc-7a3bd9736c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ed35c8-3ce6-49ee-8a24-429a0ea79105",
   "metadata": {},
   "source": [
    "#Q6 How do you handle missing values in KNN?\n",
    "\n",
    "Handling Missing Values in KNN:\n",
    "Impute missing values before applying KNN. Simple imputation methods include mean, median, or KNN imputation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a77d674-e89f-4983-b7f5-3d51af9cfb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming X_train has missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab02a8-3239-44dd-8bc4-cadb758ba80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37792921-b799-46bc-b5f7-be7bcc15a7d0",
   "metadata": {},
   "source": [
    "#Q7: Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "Comparison of KNN Classifier and Regressor:\n",
    "\n",
    "It depends on the problem:\n",
    "\n",
    "Use KNN Classifier for categorical target variables.\n",
    "\n",
    "Use KNN Regressor for continuous target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99747d25-425a-44b1-bb7b-60d1064258dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Mean Squared Error: 3364.3932584269664\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "\n",
    "# Example for classification\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize KNN Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "#Example for Regressor\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load diabetes Housing dataset\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize KNN Regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "# Train the model\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2889d3-d012-48da-97cb-565c888c40fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55629e6e-7146-43e2-a58c-e90b88a6b256",
   "metadata": {},
   "source": [
    "#Q8: What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed? \n",
    "\n",
    "Strengths and Weaknesses of KNN:\n",
    "\n",
    "Strengths: Simple, easy to understand, and effective for small to medium-sized datasets.\n",
    "\n",
    "Weaknesses: Sensitive to outliers, computation-intensive for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30d64b91-c565-437f-94b6-91e3b3120ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "# Example of handling outliers\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "outlier_detector = LocalOutlierFactor(n_neighbors=20)\n",
    "outliers = outlier_detector.fit_predict(X_train)\n",
    "X_train_no_outliers = X_train[outliers == 1]\n",
    "y_train_no_outliers = y_train[outliers == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a730dc-a1a8-4f03-80a0-a6428c3a2e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f944855-d3ae-4ba5-8bcb-6460585539e9",
   "metadata": {},
   "source": [
    "#Q9 What is the difference between Euclidean distance and Manhattan distance in KNN? \n",
    "\n",
    "Difference between Euclidean and Manhattan distance:\n",
    "\n",
    "Euclidean distance: Straight-line distance between two points.\n",
    "\n",
    "Manhattan distance: Sum of the absolute differences between corresponding coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b773109-a5a2-4c7b-8101-f0493af8a06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 5.196152422706632\n",
      "Manhattan Distance: 9\n"
     ]
    }
   ],
   "source": [
    "#9 \n",
    "# Example using scipy for distance calculation\n",
    "from scipy.spatial.distance import euclidean, cityblock\n",
    "\n",
    "point1 = [1, 2, 3]\n",
    "point2 = [4, 5, 6]\n",
    "\n",
    "euclidean_distance = euclidean(point1, point2)\n",
    "manhattan_distance = cityblock(point1, point2)\n",
    "\n",
    "print(f\"Euclidean Distance: {euclidean_distance}\")\n",
    "print(f\"Manhattan Distance: {manhattan_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffac22a-1e21-4a97-b1a7-8a92c8eee806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e01cd76d-3312-4150-a44b-d0b4f103ce1f",
   "metadata": {},
   "source": [
    "#Q10  What is the role of feature scaling in KNN?\n",
    "\n",
    "Role of Feature Scaling in KNN:\n",
    "Feature scaling ensures that all features contribute equally to the distance computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "727f4d7e-b078-41ed-bb19-3347939bc8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Scaling: 0.0\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train KNN on scaled data\n",
    "knn_classifier_scaled = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier_scaled.fit(X_train_scaled, y_train)\n",
    "accuracy_scaled = knn_classifier_scaled.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy with Scaling: {accuracy_scaled}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895715a-83b5-4e8c-a3bf-3d808ddf14f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
