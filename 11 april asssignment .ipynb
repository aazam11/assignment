{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f38899-7b89-4140-a4dd-32dbb357ed9c",
   "metadata": {},
   "source": [
    "#Q1  What is an ensemble technique in machine learning\n",
    "\n",
    "Ensemble techniques in machine learning involve combining the predictions of multiple models to improve overall performance. One common ensemble technique is the Random Forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada147a9-219e-418c-908a-70caf4916c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Random Forest Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06c4bf-2d0c-4949-9efe-90b1ed479cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "500c2787-e62e-44cd-9222-3922398c53bb",
   "metadata": {},
   "source": [
    "#Q2  Why are ensemble techniques used in machine learning?\n",
    "\n",
    "Ensemble techniques are used in machine learning for several reasons, including improving model generalization, reducing overfitting, and increasing predictive performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc182c6d-cf05-4b3e-bb4b-fbcc633e276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "gb_predictions = gb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "gb_accuracy = accuracy_score(y_test, gb_predictions)\n",
    "print(f'Gradient Boosting Accuracy: {gb_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492169d3-5bd0-4edc-99b2-43d23b5e239b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "321e563a-d26b-4ebd-990a-6596fd8047b3",
   "metadata": {},
   "source": [
    "#Q3 What is bagging?\n",
    "\n",
    "Bagging (Bootstrap Aggregating) is an ensemble technique that involves training multiple instances of the same learning algorithm on different subsets of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a29d80-3c90-4623-8917-239c9b2380b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a base Decision Tree classifier\n",
    "base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a BaggingClassifier with 100 base classifiers\n",
    "bag_classifier = BaggingClassifier(base_classifier, n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "bag_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "bag_predictions = bag_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "bag_accuracy = accuracy_score(y_test, bag_predictions)\n",
    "print(f'Bagging Classifier Accuracy: {bag_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cd93c-e523-4eb5-bbe1-b6ae9c37993b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57316c9c-13d9-4d18-9c3a-0fea325cdee2",
   "metadata": {},
   "source": [
    "#Q4\n",
    "What is boosting?\n",
    "\n",
    "Boosting is an ensemble technique that combines weak learners into a strong learner. AdaBoost is a popular boosting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492d00a7-eb52-4c27-a0da-445d45a23e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create a base Decision Tree classifier\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Create an AdaBoostClassifier with 50 base classifiers\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "adaboost_predictions = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "adaboost_accuracy = accuracy_score(y_test, adaboost_predictions)\n",
    "print(f'AdaBoost Classifier Accuracy: {adaboost_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fdefb-ff68-4120-b59e-1d61cb9f990b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "255de6d5-7265-4e33-8ec5-d9522496b34e",
   "metadata": {},
   "source": [
    "#Q5 What are the benefits of using ensemble techniques?\n",
    "\n",
    "Improved model generalization.\n",
    "\n",
    "Reduction of overfitting.\n",
    "\n",
    "Increased predictive accuracy.\n",
    "\n",
    "Robustness to noisy data.\n",
    "\n",
    "Better performance on complex datasets.\n",
    "\n",
    "Adaptability to various types of base learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912576ce-40a3-4978-be9f-e300da5e5516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d85c584-ecfc-4cbc-977c-187714227b5b",
   "metadata": {},
   "source": [
    "#Q6\n",
    " Are ensemble techniques always better than individual models?\n",
    "\n",
    "Ensemble techniques are not always better, but they often outperform individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b23e46e-d906-4f07-9832-b3f3cfc21023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Random Forest Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Fit the Decision Tree on the training data\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data using Decision Tree\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of Decision Tree\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "print(f'Decision Tree Accuracy: {dt_accuracy}')\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the Random Forest on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data using Random Forest\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of Random Forest\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(f'Random Forest Accuracy: {rf_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c53698-6f7a-4e14-9dd3-553c920d00f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b261c2b-6171-4078-b294-3d4aec2d249e",
   "metadata": {},
   "source": [
    "#Q7\n",
    "Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic. Confidence intervals can be calculated by determining the range of values that encompass a specified percentage of the bootstrap resampling distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd630808-0adc-46ca-8c1c-2b83c76aa283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval (95.0%):\n",
      "Lower Bound: 4.4553\n",
      "Upper Bound: 5.1257\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Generate some example data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=5, scale=2, size=100)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap_samples = 1000\n",
    "\n",
    "# Bootstrap resampling and calculation of means\n",
    "bootstrap_means = []\n",
    "for _ in range(n_bootstrap_samples):\n",
    "    bootstrap_sample = resample(data)\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate the confidence interval\n",
    "confidence_level = 0.95\n",
    "lower_bound = np.percentile(bootstrap_means, (1 - confidence_level) / 2 * 100)\n",
    "upper_bound = np.percentile(bootstrap_means, (1 + confidence_level) / 2 * 100)\n",
    "\n",
    "print(f\"Bootstrap Confidence Interval ({confidence_level * 100:.1f}%):\")\n",
    "print(f\"Lower Bound: {lower_bound:.4f}\")\n",
    "print(f\"Upper Bound: {upper_bound:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9706d-6b43-4e6d-9784-ab02014846e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fda3aaef-4e4e-45ab-a667-e7babd182677",
   "metadata": {},
   "source": [
    "#Q8\n",
    "How does bootstrap work, and what are the steps involved in bootstrap? \n",
    "\n",
    "Bootstrap is a resampling technique that involves creating multiple samples with replacement from the original dataset. The steps involved in bootstrap are:\n",
    "\n",
    "Sample with Replacement: Randomly select samples from the dataset with replacement.\n",
    "\n",
    "Perform Analysis: Apply the analysis (e.g., calculate mean, confidence interval) to each bootstrap sample.\n",
    "\n",
    "Aggregate Results: Aggregate the results from the analyses to estimate the population parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52546a26-d9a2-4b8f-9557-378ccb9fc855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Mean: 5.4744\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "original_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "num_iterations = 1000\n",
    "\n",
    "# Function to calculate the mean of a sample\n",
    "def calculate_mean(sample):\n",
    "    return np.mean(sample)\n",
    "\n",
    "# Bootstrap procedure\n",
    "means = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Resample with replacement\n",
    "    bootstrap_sample = np.random.choice(original_data, size=len(original_data), replace=True)\n",
    "    \n",
    "    # Calculate mean of the resampled data\n",
    "    mean = calculate_mean(bootstrap_sample)\n",
    "    means.append(mean)\n",
    "\n",
    "# Calculate the mean of means\n",
    "bootstrap_mean = np.mean(means)\n",
    "print(f'Bootstrap Mean: {bootstrap_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bacb26-7091-417c-80b1-a8a763f61dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48169e0-05ec-4aa6-b7f3-00e0aa4fdc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Confidence Interval (95.0%): [14.673672947308443, 15.824235623812122]\n"
     ]
    }
   ],
   "source": [
    "#Q9\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "sample_heights = np.random.normal(loc=15, scale=2, size=50)  # Simulating a sample with a mean of 15 and std deviation of 2\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "num_iterations = 1000\n",
    "\n",
    "# Function to calculate the mean of a sample\n",
    "def calculate_mean(sample):\n",
    "    return np.mean(sample)\n",
    "\n",
    "# Function to perform bootstrap and calculate confidence interval\n",
    "def bootstrap_confidence_interval(data, num_iterations, confidence_level):\n",
    "    means = []\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Resample with replacement\n",
    "        bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        \n",
    "        # Calculate mean of the resampled data\n",
    "        mean = calculate_mean(bootstrap_sample)\n",
    "        means.append(mean)\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    lower_bound = np.percentile(means, (1 - confidence_level) / 2 * 100)\n",
    "    upper_bound = np.percentile(means, (1 + confidence_level) / 2 * 100)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate confidence interval using bootstrap\n",
    "confidence_level = 0.95\n",
    "lower, upper = bootstrap_confidence_interval(sample_heights, num_iterations, confidence_level)\n",
    "\n",
    "print(f'Bootstrap Confidence Interval ({confidence_level*100}%): [{lower}, {upper}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d08a8-953c-4a01-900a-6110439d60f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
