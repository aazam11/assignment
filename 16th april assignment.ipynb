{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015f4f22-abe8-40c4-9079-4945910554e9",
   "metadata": {},
   "source": [
    "#Q1. What is boosting in machine learning?\n",
    "\n",
    "Boosting is an ensemble learning technique where multiple weak learners (models that perform slightly better than random chance) are combined to form a strong learner. The idea is to train each weak learner sequentially, with each new learner focusing on the mistakes made by the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b074f7-510d-433f-9d6c-9f9ec03123f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "#1 \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree as the base learner\n",
    "base_learner = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create an AdaBoost classifier with 50 weak learners\n",
    "adaboost_classifier = AdaBoostClassifier(base_learner, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b67b65-d6a0-42ef-9cd1-adf36a88ef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "859babad-9a63-4be5-93d6-655c6153ca17",
   "metadata": {},
   "source": [
    "#Q2. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "Advantages of Boosting:\n",
    "\n",
    "Improved Accuracy: Boosting often leads to higher accuracy compared to individual weak learners.\n",
    "\n",
    "Handles Complex Relationships: Boosting can capture complex relationships in the data.\n",
    "\n",
    "Reduces Overfitting: It tends to reduce overfitting, especially when using weak learners with limited complexity.\n",
    "\n",
    "Feature Importance: Boosting algorithms provide insights into feature importance.\n",
    "\n",
    "Limitations of Boosting:\n",
    "\n",
    "Sensitivity to Noisy Data: Boosting can be sensitive to noisy data and outliers.\n",
    "\n",
    "Computational Complexity: Training can be computationally expensive, especially with a large number of weak learners.\n",
    "\n",
    "Interpretability: The resulting models can be complex and less interpretable compared to simpler models.\n",
    "\n",
    "Need for Tuning: Boosting algorithms often require careful tuning of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bcc78c-54ae-4ca4-ac1b-2989337762b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b9d1fae-324f-46c8-ad91-5a2c4ab97216",
   "metadata": {},
   "source": [
    "#Q3. Explain how boosting works.\n",
    "\n",
    "How Boosting Works:\n",
    "\n",
    "Initialize Weights: Assign equal weights to all training examples.\n",
    "\n",
    "Train Weak Learner: Train a weak learner on the data, and compute the error.\n",
    "\n",
    "Increase Weights for Errors: Increase the weights of the misclassified examples.\n",
    "\n",
    "Train Next Weak Learner: Train another weak learner with the updated weights.\n",
    "\n",
    "Repeat: Repeat steps 3 and 4 until a specified number of weak learners are trained.\n",
    "\n",
    "Combine Weak Learners: Combine the weak learners, giving more weight to those with lower error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0adc63d-7d18-4fcc-9ce5-d347e0a5cecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree as the base learner\n",
    "base_learner = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create an AdaBoost classifier with 50 weak learners\n",
    "adaboost_classifier = AdaBoostClassifier(base_learner, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143ce61-df52-4c01-9469-3b8bd2d69c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9c50d65-ff01-47a4-af10-ac25e4c39703",
   "metadata": {},
   "source": [
    "#Q4 What are the different types of boosting algorithms?\n",
    "\n",
    "Different Types of Boosting Algorithms:\n",
    "\n",
    "AdaBoost (Adaptive Boosting)\n",
    "\n",
    "Gradient Boosting (e.g., XGBoost, LightGBM)\n",
    "\n",
    "LogitBoost\n",
    "\n",
    "BrownBoost\n",
    "\n",
    "LPBoost (Linear Programming Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b28a37b-aaec-47f4-8941-bb6aa3daaa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "#4 example\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd41fa-d6ea-4914-870c-517d37d209b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ddacb05-e3b7-485d-b435-464bf702dcb5",
   "metadata": {},
   "source": [
    "#Q5 What are some common parameters in boosting algorithms?\n",
    "\n",
    "Common Parameters in Boosting Algorithms:\n",
    "\n",
    "n_estimators: Number of weak learners.\n",
    "\n",
    "learning_rate: Shrinks the contribution of each weak learner.\n",
    "\n",
    "max_depth: Maximum depth of weak learners (e.g., decision trees).\n",
    "\n",
    "subsample: Fraction of samples used for training each weak learner.\n",
    "\n",
    "loss: Loss function to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8282b76a-f85e-4df2-adff-c1b213f62f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create AdaBoost classifier with custom parameters\n",
    "adaboost_classifier = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9537ad-fab1-4489-889e-e3205b9975f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75f09248-8264-45e2-8664-e9d9f3d23921",
   "metadata": {},
   "source": [
    "#Q6 How do boosting algorithms combine weak learners to create a strong learner?\n",
    "\n",
    "Combining Weak Learners:\n",
    "\n",
    "Weighted Sum: Assign a weight to each weak learner based on its performance.\n",
    "\n",
    "Voting: Allow each weak learner to vote, and the final prediction is based on a majority vote or a weighted vote.\n",
    "\n",
    "Example:\n",
    "In AdaBoost, each weak learner is assigned a weight based on its performance in reducing misclassification error. The final prediction is a weighted sum of the weak learners' predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492fcd0-3f8d-4e9f-91cc-7b7db85ff75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b6af082-30ce-4e4c-847b-45d4797b4166",
   "metadata": {},
   "source": [
    "#Q7 Explain the concept of AdaBoost algorithm and its working.\n",
    "\n",
    "AdaBoost Algorithm:\n",
    "\n",
    "Initialize sample weights.\n",
    "\n",
    "For each weak learner:\n",
    "a. Train a weak learner with the current sample weights.\n",
    "\n",
    "b. Compute the error of the weak learner.\n",
    "\n",
    "c. Compute the weight of the weak learner based on its error.\n",
    "\n",
    "d. Update the sample weights, giving more weight to misclassified examples.\n",
    "\n",
    "Combine weak learners with weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07197d2f-6046-4444-9d94-42ac04d14d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create AdaBoost classifier with decision trees as weak learners\n",
    "adaboost_classifier = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4f0f5-c116-437d-89bb-44ea14bf55c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48727825-558a-481e-baf3-6b5f933168d4",
   "metadata": {},
   "source": [
    "#Q8. What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "In AdaBoost, the loss function is not explicitly specified as it is in some other algorithms like gradient boosting. However, AdaBoost implicitly minimizes an exponential loss function. The key idea is to assign higher weights to misclassified samples, making them more influential in the training of subsequent weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01240da7-480d-4f7e-a92f-bc4842ed344c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree as the base learner\n",
    "base_learner = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create an AdaBoost classifier with 50 weak learners\n",
    "adaboost_classifier = AdaBoostClassifier(base_learner, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b112b-8d33-487f-a0e1-ab56bf0715e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0f3cc78-b2f1-4315-a435-688a3831f44f",
   "metadata": {},
   "source": [
    "#Q9 How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "\n",
    "Updating Weights in AdaBoost:\n",
    "\n",
    "The weights of misclassified samples are increased to give them more influence in the subsequent training of weak learners.\n",
    "\n",
    "a simplified explanation of how AdaBoost updates weights:\n",
    "\n",
    "Initialize Weights: Assign equal weights to all training samples.\n",
    "\n",
    "Train Weak Learner: Train a weak learner (e.g., decision tree) on the weighted training set.\n",
    "\n",
    "Compute Error: Compute the error of the weak learner on the training set. The error is the sum of weights of misclassified samples.\n",
    "\n",
    "Compute Weak Learner Weight: Compute the weight of the weak learner in the final combination based on its error. Lower error leads to higher weight.\n",
    "\n",
    "Update Weights: Increase the weights of misclassified samples so that they are more likely to be selected in the next iteration.\n",
    "\n",
    "Repeat: Repeat steps 2-5 for a predefined number of iterations or until a specified level of accuracy is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04add255-6d5f-47f3-84de-4d857e260e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "Weak learner 1 weight: 1.0\n",
      "Weak learner 2 weight: 1.0\n",
      "Weak learner 3 weight: 1.0\n",
      "Weak learner 4 weight: 1.0\n",
      "Weak learner 5 weight: 1.0\n",
      "Weak learner 6 weight: 1.0\n",
      "Weak learner 7 weight: 1.0\n",
      "Weak learner 8 weight: 1.0\n",
      "Weak learner 9 weight: 1.0\n",
      "Weak learner 10 weight: 1.0\n",
      "Weak learner 11 weight: 1.0\n",
      "Weak learner 12 weight: 1.0\n",
      "Weak learner 13 weight: 1.0\n",
      "Weak learner 14 weight: 1.0\n",
      "Weak learner 15 weight: 1.0\n",
      "Weak learner 16 weight: 1.0\n",
      "Weak learner 17 weight: 1.0\n",
      "Weak learner 18 weight: 1.0\n",
      "Weak learner 19 weight: 1.0\n",
      "Weak learner 20 weight: 1.0\n",
      "Weak learner 21 weight: 1.0\n",
      "Weak learner 22 weight: 1.0\n",
      "Weak learner 23 weight: 1.0\n",
      "Weak learner 24 weight: 1.0\n",
      "Weak learner 25 weight: 1.0\n",
      "Weak learner 26 weight: 1.0\n",
      "Weak learner 27 weight: 1.0\n",
      "Weak learner 28 weight: 1.0\n",
      "Weak learner 29 weight: 1.0\n",
      "Weak learner 30 weight: 1.0\n",
      "Weak learner 31 weight: 1.0\n",
      "Weak learner 32 weight: 1.0\n",
      "Weak learner 33 weight: 1.0\n",
      "Weak learner 34 weight: 1.0\n",
      "Weak learner 35 weight: 1.0\n",
      "Weak learner 36 weight: 1.0\n",
      "Weak learner 37 weight: 1.0\n",
      "Weak learner 38 weight: 1.0\n",
      "Weak learner 39 weight: 1.0\n",
      "Weak learner 40 weight: 1.0\n",
      "Weak learner 41 weight: 1.0\n",
      "Weak learner 42 weight: 1.0\n",
      "Weak learner 43 weight: 1.0\n",
      "Weak learner 44 weight: 1.0\n",
      "Weak learner 45 weight: 1.0\n",
      "Weak learner 46 weight: 1.0\n",
      "Weak learner 47 weight: 1.0\n",
      "Weak learner 48 weight: 1.0\n",
      "Weak learner 49 weight: 1.0\n",
      "Weak learner 50 weight: 1.0\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize AdaBoost with decision trees as weak learners\n",
    "adaboost = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\n",
    "\n",
    "# Train AdaBoost\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# You can also access the individual weak learners and their weights\n",
    "for i, estimator in enumerate(adaboost.estimators_):\n",
    "    print(f\"Weak learner {i+1} weight: {adaboost.estimator_weights_[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2ff1a-be7c-49ce-a5bb-a1d5510525a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c964dd83-8f5c-4eaa-a3c2-1132c8689048",
   "metadata": {},
   "source": [
    "#Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm? give a practical example with code\n",
    "\n",
    "Effect of Increasing Estimators:\n",
    "\n",
    "Increasing the number of estimators (weak learners) in AdaBoost typically improves the model's performance up to a point. However, there's a diminishing return, and adding too many weak learners may lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b785237-36cd-4650-b584-03728b1dd6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 10 estimators: 0.85\n",
      "Accuracy with 50 estimators: 0.87\n",
      "Accuracy with 100 estimators: 0.855\n",
      "Accuracy with 200 estimators: 0.855\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "# ... (previous code)\n",
    "# Vary the number of estimators and observe the effect on accuracy\n",
    "\n",
    "for num_estimators in [10, 50, 100, 200]:\n",
    "    adaboost_classifier = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=num_estimators,\n",
    "        learning_rate=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    adaboost_classifier.fit(X_train, y_train)\n",
    "    y_pred = adaboost_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with {num_estimators} estimators: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
