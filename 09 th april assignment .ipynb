{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ace556-3530-42f3-9106-597561689a73",
   "metadata": {},
   "source": [
    "\n",
    "#Q1/2.\n",
    "What is Bayes' theorem? \n",
    "\n",
    "Bayes' theorem is a mathematical formula that describes the probability of an event, based on prior knowledge of conditions that might be related to the event. It is named after Thomas Bayes. The formula is given by:\n",
    "\n",
    "P(A∣B)= P(B∣A)⋅P(A)/P(B)\n",
    "\n",
    "where:\n",
    "P(A∣B) is the probability of event A given that event B has occurred.\n",
    "\n",
    "P(B∣A) is the probability of event B given that event A has occurred.\n",
    "\n",
    "P(A) and P(B) are the probabilities of events A and B, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418d936-790e-4a7b-8c4f-630204526ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb805e74-3e70-46c2-8cbc-4da60e409fd6",
   "metadata": {},
   "source": [
    "#Q3\n",
    "Bayes' theorem is used in various fields, particularly in Bayesian statistics and machine learning. One practical example is in spam email classification. Let's consider a scenario where we want to classify emails as either spam or non-spam (ham) using Bayes' theorem. We can use the Naive Bayes algorithm for this classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c3ac4b-0299-4fbe-bec4-588c388cd234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7692307692307693\n",
      "Confusion Matrix:\n",
      " [[139  21]\n",
      " [ 45  81]]\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=['alt.atheism', 'talk.religion.misc'], remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Separate data into features (X) and labels (y)\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data into numerical features using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff47cdc-5c71-486d-9b75-1c34aefcf0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fca2374-643a-4aba-bd7e-2b01a9288757",
   "metadata": {},
   "source": [
    "#Q4\n",
    "Bayes' theorem and conditional probability are closely related. Bayes' theorem is a mathematical formula that describes how to update the probability of a hypothesis based on new evidence, and it involves conditional probabilities.\n",
    "\n",
    "The relationship between Bayes' theorem and conditional probability can be illustrated through the following mathematical expression:\n",
    "\n",
    "P(A∣B)= P(B∣A)⋅P(A)/P(B)\n",
    "\n",
    "Here, P(A∣B) is the conditional probability of event A given that event B has occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d1fa25-3afa-4730-9e58-f317bbf97f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Probability of A given B: 0.44999999999999996\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "import numpy as np\n",
    "\n",
    "# Define conditional probabilities\n",
    "P_A = 0.3  # Probability of event A\n",
    "P_B_given_A = 0.6  # Probability of event B given A\n",
    "\n",
    "# Calculate the unconditional probability of B\n",
    "P_B = 0.4  # Probability of event B (unconditional)\n",
    "\n",
    "# Apply Bayes' theorem to calculate P(A|B)\n",
    "P_A_given_B = (P_B_given_A * P_A) / P_B\n",
    "\n",
    "print(\"Conditional Probability of A given B:\", P_A_given_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0a6a7-7c52-449b-9312-f0a4b647effc",
   "metadata": {},
   "source": [
    "In this example, we have:\n",
    "\n",
    "P(A): Probability of event A occurring (e.g., prior belief).\n",
    "\n",
    "P(B∣A): Probability of event B occurring given that A has occurred (evidence supporting A).\n",
    "\n",
    "P(B): Probability of event B occurring (unconditional probability).\n",
    "\n",
    "We then use Bayes' theorem to calculate P(A∣B), which is the updated probability of A given the new evidence B. This relationship is fundamental in Bayesian statistics, where prior beliefs are updated based on observed evidence.\n",
    "\n",
    "Note: In real-world scenarios, these probabilities may represent the likelihood of hypotheses, events, or outcomes in various contexts, such as machine learning, medical diagnosis, or finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51d7e2-70c7-420a-bccd-0e76a234d502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b89994-36f1-4b86-b138-cef0b5905899",
   "metadata": {},
   "source": [
    "#Q5\n",
    "The choice of which type of Naive Bayes classifier to use for a given problem depends on the nature of the features and the underlying assumptions about the data. The three main types of Naive Bayes classifiers are:\n",
    "\n",
    "Gaussian Naive Bayes: Assumes that the numerical features follow a Gaussian (normal) distribution.\n",
    "\n",
    "Multinomial Naive Bayes: Suitable for discrete data, often used for document classification tasks where features represent word counts or term frequencies.\n",
    "\n",
    "Bernoulli Naive Bayes: Appropriate when dealing with binary or Boolean features.\n",
    "\n",
    "Let's illustrate the choice with a practical example using the famous Iris dataset, which is commonly used for classification tasks. We'll compare the performance of Gaussian, Multinomial, and Bernoulli Naive Bayes classifiers on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9efcde72-b248-4a72-ad34-696d47e7a148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Accuracy: 1.0\n",
      "Multinomial Naive Bayes Accuracy: 0.9\n",
      "Bernoulli Naive Bayes Accuracy: 0.3\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_predictions = gnb.predict(X_test)\n",
    "gnb_accuracy = accuracy_score(y_test, gnb_predictions)\n",
    "print(\"Gaussian Naive Bayes Accuracy:\", gnb_accuracy)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "mnb_predictions = mnb.predict(X_test)\n",
    "mnb_accuracy = accuracy_score(y_test, mnb_predictions)\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", mnb_accuracy)\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "# For the purpose of this example, we need to discretize the features for Bernoulli NB\n",
    "X_train_binary = (X_train > 0).astype(int)\n",
    "X_test_binary = (X_test > 0).astype(int)\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_binary, y_train)\n",
    "bnb_predictions = bnb.predict(X_test_binary)\n",
    "bnb_accuracy = accuracy_score(y_test, bnb_predictions)\n",
    "print(\"Bernoulli Naive Bayes Accuracy:\", bnb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcee68-b4d7-4cb1-9888-cfb60d6f2baa",
   "metadata": {},
   "source": [
    "In this example, the choice of Naive Bayes classifier depends on the type of features in the Iris dataset. Since the Iris dataset contains continuous numerical features, we use Gaussian Naive Bayes, which assumes a Gaussian distribution. If the features were discrete (e.g., counts or frequencies), Multinomial Naive Bayes might be more suitable. For binary features, Bernoulli Naive Bayes could be considered.\n",
    "\n",
    "It's crucial to understand the nature of the data and the assumptions made by each Naive Bayes classifier to make an informed choice. In practice, it's often a good idea to try different classifiers and evaluate their performance on the specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e39ec-a3c6-4d3f-9f9f-deb9421036fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093c4138-6424-4143-b19a-cc23a5ea7d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: B\n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training data\n",
    "X_train = [\n",
    "    [1, 1], [1, 2], [1, 2], [1, 3], [1, 3], [1, 3], [1, 4],\n",
    "    [2, 1], [2, 2], [2, 2], [2, 3], [2, 3], [2, 3], [2, 4],\n",
    "]\n",
    "y_train = ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B']\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# New instance for prediction\n",
    "new_instance = [[3, 4]]\n",
    "\n",
    "# Predict the class for the new instance\n",
    "predicted_class = classifier.predict(new_instance)\n",
    "\n",
    "print(\"Predicted class:\", predicted_class[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ff8b1-3e45-4b93-bd55-a313ea44991c",
   "metadata": {},
   "source": [
    "we use a training dataset (X_train and y_train) with two features (X1 and X2) and corresponding class labels (A and B). We then create a Multinomial Naive Bayes classifier, train it with the training data, and predict the class for the new instance with features X1=3 and X2=4. The predicted class is printed as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d6d97-7d6f-4ced-bf91-a566f60f4575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
