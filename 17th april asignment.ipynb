{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c78b10-27c3-445b-a590-29457c41f5b9",
   "metadata": {},
   "source": [
    "#Q1. What is Gradient Boosting Regression? Give a practical example with code:\n",
    "\n",
    "Gradient Boosting Regression is an ensemble learning technique that combines the predictions from multiple weak learners (typically decision trees) to create a strong predictive model. It builds the model sequentially, with each new tree trying to correct the errors of the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80417197-dc7c-4340-b666-0b5f925d123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.7340600217411903\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "# Import necessary libraries\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Generate some example data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd472f6-6169-4d28-823d-83aefa0d00fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cac2b47-20d7-4ce8-990f-4f3fdcb88ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.7340600217411903\n",
      "R-squared: 0.044479095570065574\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def gradient_boosting(X, y, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "    # Initialize predictions with the mean of the target variable\n",
    "    predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "    for _ in range(n_estimators):\n",
    "        # Compute residuals\n",
    "        residuals = y - predictions\n",
    "\n",
    "        # Fit a decision tree to the residuals\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        tree.fit(X, residuals)\n",
    "\n",
    "        # Update predictions with the learning rate and the new tree's predictions\n",
    "        predictions += learning_rate * tree.predict(X)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "# Generate some example data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.randn(100)\n",
    "\n",
    "# Use the gradient boosting function\n",
    "predictions = gradient_boosting(X, y)\n",
    "\n",
    "# Train the model\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d8f3c-fdec-4039-b757-fdc13af1063a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81dc6f68-e89d-4281-971c-004a8f98804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Mean Squared Error with Best Hyperparameters: 0.5973702258607219\n"
     ]
    }
   ],
   "source": [
    "#Q3\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_gb_regressor = GradientBoostingRegressor(**best_params, random_state=42)\n",
    "best_gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "best_mse = mean_squared_error(y_test, best_predictions)\n",
    "print(f\"Mean Squared Error with Best Hyperparameters: {best_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f8510d-b2da-4528-be4c-c72809278c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a288b982-45f9-45a7-a214-5458b259cf87",
   "metadata": {},
   "source": [
    "#Q4. What is a weak learner in Gradient Boosting? Give a practical example with code:\n",
    "\n",
    "A weak learner is a model that performs slightly better than random chance. In the context of gradient boosting, decision trees with shallow depths are commonly used as weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840969b3-a016-473d-9ad6-646125c9d581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of Weak Learner: 0.6054414730725821\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "# Example of a weak learner using a shallow decision tree\n",
    "weak_learner = DecisionTreeRegressor(max_depth=2)\n",
    "weak_learner.fit(X_train, y_train)\n",
    "weak_predictions = weak_learner.predict(X_test)\n",
    "\n",
    "weak_mse = mean_squared_error(y_test, weak_predictions)\n",
    "print(f\"Mean Squared Error of Weak Learner: {weak_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30108460-7ef8-49ae-8952-3c07b17d6656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b89184b-f1c1-4746-a0c8-b00a35f25e12",
   "metadata": {},
   "source": [
    "#Q5 What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to sequentially add weak learners (typically shallow decision trees) to correct the errors made by the previous ones. Each tree is trained to predict the residuals (the differences between the actual and predicted values) of the ensemble, and their predictions are combined with a weighted sum. The learning process involves minimizing a loss function, often mean squared error for regression problems, by adjusting the weights and predictions of each weak learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c01cf-b66b-48e6-8d82-e2b85e486c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8833050-bfa2-43b5-9b0d-e9c8ae376ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of Gradient Boosting Ensemble: 0.7340600217411903\n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "# Build an ensemble of weak learners using scikit-learn's GradientBoostingRegressor\n",
    "ensemble_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "ensemble_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "ensemble_predictions = ensemble_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_mse = mean_squared_error(y_test, ensemble_predictions)\n",
    "print(f\"Mean Squared Error of Gradient Boosting Ensemble: {ensemble_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3198da1-398c-4e26-8bae-8416b2aeccbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c05af81b-3a95-4681-84e2-d33d26443cee",
   "metadata": {},
   "source": [
    "#Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "Initialize the Model: Start with a simple model, often the mean of the target variable for regression problems.\n",
    "\n",
    "Compute Residuals: Calculate the difference between the actual and predicted values (residuals).\n",
    "\n",
    "Fit a Weak Learner to Residuals: Train a weak learner (e.g., a decision tree) to predict the residuals.\n",
    "\n",
    "Update Predictions: Update the model's predictions by adding the predictions of the weak learner, scaled by a learning rate.\n",
    "\n",
    "Repeat Steps 2-4: Repeat steps 2-4 for a specified number of iterations or until a convergence criterion is met.\n",
    "\n",
    "Final Prediction: The final prediction is the sum of all weak learner predictions.\n",
    "\n",
    "The mathematical intuition involves minimizing a loss function by iteratively improving the model's predictions through the addition of weak learners. The learning rate controls the step size in each iteration, and the ensemble aims to capture complex patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
