{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81feb94-68e9-421a-8b7f-ec88cf29a2ca",
   "metadata": {},
   "source": [
    "#Q1.\n",
    "Since Naive Bayes assumes independence between features, it might not be the best model for this specific case where smoking and health insurance usage may not be completely independent. However, for the sake of the question, let's proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a3e4d3-3e93-4de3-adad-01683460c4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that an employee is a smoker: 0.27999999999999997\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Define the probabilities from the survey\n",
    "p_health_insurance = 0.7  # Probability that an employee uses the health insurance plan\n",
    "p_smoker_given_health_insurance = 0.4  # Probability that an employee is a smoker given that he/she uses the health insurance plan\n",
    "\n",
    "# Calculate the probability that an employee is a smoker\n",
    "p_smoker = p_health_insurance * p_smoker_given_health_insurance\n",
    "\n",
    "# Display the result\n",
    "print(\"Probability that an employee is a smoker:\", p_smoker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a65fb-f8f9-4eac-acb9-093518ba1dc8",
   "metadata": {},
   "source": [
    "#Note:\n",
    "this solution assumes independence between the events \"using the health insurance plan\" and \"being a smoker,\" which may not be a valid assumption in real-world scenarios.\n",
    "\n",
    "For a more accurate model, you might need more data and a more sophisticated model that considers dependencies between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd75d5-5b88-4ab8-aa2c-f22925c6b58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "500c76ae-1fe3-4552-bbff-343f07c421b1",
   "metadata": {},
   "source": [
    "#Q2\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm that are commonly used for text classification problems, such as spam detection or sentiment analysis. The key difference lies in the type of features they are designed to work with.\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Assumes that features are binary variables (0 or 1), indicating the absence or presence of a particular feature.\n",
    "Often used for document classification tasks, where each term's presence or absence in a document is considered.\n",
    "Suitable for situations where the occurrence of a feature matters more than its frequency.\n",
    "\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Assumes that features are discrete and represent counts or frequencies (non-negative integers).\n",
    "Commonly used for text classification tasks where features are word counts (bag-of-words model).\n",
    "Suitable for situations where the frequency of a feature is important, such as in natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a3a5b3-94e0-4260-a0f5-a49105b4d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Accuracy: 0.0\n",
      "Multinomial Naive Bayes Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample data\n",
    "documents = [\"I like natural language processing.\",\n",
    "             \"Naive Bayes is a simple algorithm.\",\n",
    "             \"Spam emails are annoying.\",\n",
    "             \"I dislike spam emails.\"]\n",
    "\n",
    "labels = [1, 1, 0, 0]  # 1 for positive (non-spam), 0 for negative (spam)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Vectorize the documents using the bag-of-words model\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_nb.fit(X_train_bow, y_train)\n",
    "y_pred_bernoulli = bernoulli_nb.predict(X_test_bow)\n",
    "accuracy_bernoulli = accuracy_score(y_test, y_pred_bernoulli)\n",
    "print(\"Bernoulli Naive Bayes Accuracy:\", accuracy_bernoulli)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(X_train_bow, y_train)\n",
    "y_pred_multinomial = multinomial_nb.predict(X_test_bow)\n",
    "accuracy_multinomial = accuracy_score(y_test, y_pred_multinomial)\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", accuracy_multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029c221-e588-4f7b-b8b6-5a4e6a049759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "325099ac-f69b-45b2-bf7e-e4ad274c4a3c",
   "metadata": {},
   "source": [
    "#Q3\n",
    "Bernoulli Naive Bayes assumes that features are binary variables, indicating the presence or absence of a particular feature. In the context of missing values, the algorithm can still work, but it interprets missing values as the absence of a feature. The algorithm doesn't explicitly handle missing values in the way some other algorithms might, and it might not be the best choice if missing values are common and important in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b120a286-17b5-490e-b7f9-50343c783b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='target')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a RandomForest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a Bernoulli Naive Bayes classifier (not suitable for this dataset, just for illustration)\n",
    "bnb_classifier = BernoulliNB()\n",
    "\n",
    "# Create an ensemble model with a Voting Classifier\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('bernoulli_naive_bayes', bnb_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Ensemble Model Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c181c9-0e03-43ab-a934-89bc9723577f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63645630-7a55-4515-adbb-af3b9b4d4620",
   "metadata": {},
   "source": [
    "#Q4\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. It is a variant of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution. Scikit-learn provides the GaussianNB class for Gaussian Naive Bayes, and it supports multi-class classification out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "761ad5d2-bda0-4251-ae21-9c7d1e459a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        11\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset (a well-known dataset for multi-class classification)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create and fit the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a0b07-c5c4-4ff0-b1fd-0707165cefde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9258a663-4014-49f9-9334-cdc7bb9c9cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "\n",
      "Additional Metrics:\n",
      "Bernoulli Naive Bayes - Accuracy: 0.8839380364047911 Precision: 0.8869617393737383 Recall: 0.8152389047416673 F1 Score: 0.8481249015095276\n",
      "Multinomial Naive Bayes - Accuracy: 0.7863496180326323 Precision: 0.7393175533565436 Recall: 0.7214983911116508 F1 Score: 0.7282909724016348\n",
      "Gaussian Naive Bayes - Accuracy: 0.8217730830896915 Precision: 0.7103733928118492 Recall: 0.9569516119239877 F1 Score: 0.8130660909542995\n"
     ]
    }
   ],
   "source": [
    "#Q5\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# Step 1: Load the Spambase dataset\n",
    "# Download the dataset from the UCI Machine Learning Repository and adjust the file path\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [f'feature_{i}' for i in range(57)] + ['spam']\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Step 2: Split the data into features (X) and target (y)\n",
    "X = df.drop('spam', axis=1)\n",
    "y = df['spam']\n",
    "\n",
    "# Step 3: Implement and evaluate Bernoulli Naive Bayes\n",
    "print(\"\\nBernoulli Naive Bayes:\")\n",
    "bnb = BernoulliNB()\n",
    "bnb_scores = cross_val_score(bnb, X, y, cv=10, scoring=make_scorer(accuracy_score))\n",
    "print(\"Accuracy:\", bnb_scores.mean())\n",
    "\n",
    "# Step 4: Implement and evaluate Multinomial Naive Bayes\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "mnb = MultinomialNB()\n",
    "mnb_scores = cross_val_score(mnb, X, y, cv=10, scoring=make_scorer(accuracy_score))\n",
    "print(\"Accuracy:\", mnb_scores.mean())\n",
    "\n",
    "# Step 5: Implement and evaluate Gaussian Naive Bayes\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "gnb = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb, X, y, cv=10, scoring=make_scorer(accuracy_score))\n",
    "print(\"Accuracy:\", gnb_scores.mean())\n",
    "\n",
    "# Step 6: Report additional metrics for each classifier\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring=make_scorer(accuracy_score)).mean()\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring=make_scorer(precision_score)).mean()\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring=make_scorer(recall_score)).mean()\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring=make_scorer(f1_score)).mean()\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "print(\"\\nAdditional Metrics:\")\n",
    "\n",
    "bnb_metrics = evaluate_classifier(BernoulliNB(), X, y)\n",
    "print(\"Bernoulli Naive Bayes - Accuracy:\", bnb_metrics[0], \"Precision:\", bnb_metrics[1], \"Recall:\", bnb_metrics[2], \"F1 Score:\", bnb_metrics[3])\n",
    "\n",
    "mnb_metrics = evaluate_classifier(MultinomialNB(), X, y)\n",
    "print(\"Multinomial Naive Bayes - Accuracy:\", mnb_metrics[0], \"Precision:\", mnb_metrics[1], \"Recall:\", mnb_metrics[2], \"F1 Score:\", mnb_metrics[3])\n",
    "\n",
    "gnb_metrics = evaluate_classifier(GaussianNB(), X, y)\n",
    "print(\"Gaussian Naive Bayes - Accuracy:\", gnb_metrics[0], \"Precision:\", gnb_metrics[1], \"Recall:\", gnb_metrics[2], \"F1 Score:\", gnb_metrics[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b89c4e-0fac-4c51-89f5-b3bb4a5bb201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
