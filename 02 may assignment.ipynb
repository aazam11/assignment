{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91ce9a3-b619-4976-93f2-3527b22a1af6",
   "metadata": {},
   "source": [
    "#Q1\n",
    "Anomaly detection is a technique used to identify patterns in data that do not conform to expected behavior. Its purpose is to identify outliers or anomalies in a dataset that deviate from the norm. Anomalies could represent errors, fraud, or other unexpected events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70fe2d05-6e4b-4def-b14c-fe9a87fec099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with anomalies\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.normal(loc=0, scale=1, size=(1000, 2))\n",
    "anomalies = np.random.normal(loc=5, scale=1, size=(50, 2))\n",
    "data = np.vstack([normal_data, anomalies])\n",
    "\n",
    "# Fit an Isolation Forest model\n",
    "model = IsolationForest(contamination=0.05)  # 5% contamination (expected proportion of anomalies)\n",
    "model.fit(data)\n",
    "\n",
    "# Predict anomalies (1 for normal, -1 for anomalies)\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a05fd-0343-4c63-a406-bb0988965f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "438a083d-f99b-447e-9168-73ff19ea980b",
   "metadata": {},
   "source": [
    "#Q2\n",
    "Key challenges in anomaly detection include dealing with imbalanced datasets, adapting to evolving patterns, and selecting appropriate features. Here's an example using a dataset with imbalanced classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d57ac2b6-a317-40ee-a2e9-c72b12d302ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1  1  1\n",
      " -1 -1  1  1 -1  1 -1  1 -1  1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  1 -1 -1 -1  1 -1  1 -1  1  1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  1\n",
      " -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1  1  1\n",
      " -1 -1  1  1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.normal(loc=0, scale=1, size=(900, 2))\n",
    "anomalies = np.random.normal(loc=5, scale=1, size=(100, 2))\n",
    "data = np.vstack([normal_data, anomalies])\n",
    "\n",
    "# Fit an Isolation Forest model\n",
    "model = IsolationForest(contamination=0.1)  # 10% contamination (expected proportion of anomalies)\n",
    "model.fit(data)\n",
    "\n",
    "# Predict anomalies (1 for normal, -1 for anomalies)\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d2f64-21e2-4738-b550-e9eb444e736b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f26b280-2d76-48b8-8d80-3b8b40da2efa",
   "metadata": {},
   "source": [
    "#Q3\n",
    "Unsupervised anomaly detection doesn't require labeled data, while supervised anomaly detection relies on labeled examples. Example using Isolation Forest for unsupervised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe432d37-7a8e-49ec-8e18-62851628f81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1 -1  1  1\n",
      "  1  1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1 -1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1\n",
      "  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      " -1  1  1 -1  1  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset (no labels)\n",
    "data = np.random.normal(loc=0, scale=1, size=(1000, 2))\n",
    "\n",
    "# Fit an Isolation Forest model\n",
    "model = IsolationForest(contamination=0.05)  # 5% contamination (expected proportion of anomalies)\n",
    "model.fit(data)\n",
    "\n",
    "# Predict anomalies (1 for normal, -1 for anomalies)\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66d2b2-4c77-4998-a2fb-22b3819d119b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd94c7fa-ece1-4355-9318-a784159c964e",
   "metadata": {},
   "source": [
    "#Q4\n",
    "Main categories include statistical methods, machine learning-based methods, and proximity-based methods. Example using a proximity-based method (Isolation Forest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f9adcf-792a-4f65-9eba-3fa8bc3023d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1  1  1 -1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1\n",
      "  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1  1  1  1 -1  1  1  1  1 -1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.random.normal(loc=0, scale=1, size=(1000, 2))\n",
    "\n",
    "# Fit an Isolation Forest model\n",
    "model = IsolationForest(contamination=0.05)  # 5% contamination (expected proportion of anomalies)\n",
    "model.fit(data)\n",
    "\n",
    "# Predict anomalies (1 for normal, -1 for anomalies)\n",
    "predictions = model.predict(data)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c3da8-b165-4faf-835d-f4be89428cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0faa0c69-6f04-4e3e-9a72-58e2f0ef4406",
   "metadata": {},
   "source": [
    "#Q5\n",
    "Distance-based anomaly detection methods assume that anomalies are far from normal instances. Example using K-Nearest Neighbors (KNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e80030af-f016-4098-b3fd-56152adbcdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.05866551 0.06227732 0.06818919 0.07515834]\n",
      " [0.         0.1647693  0.17702975 0.21068721 0.21174465]\n",
      " [0.         0.01413078 0.02859833 0.03819789 0.09033897]\n",
      " ...\n",
      " [0.         0.01994344 0.10868608 0.14387276 0.15376172]\n",
      " [0.         0.03694488 0.0499535  0.06750045 0.07681045]\n",
      " [0.         0.04356882 0.0817513  0.0971224  0.09716311]]\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.random.normal(loc=0, scale=1, size=(1000, 2))\n",
    "\n",
    "# Fit a KNN model\n",
    "knn = NearestNeighbors(n_neighbors=5)\n",
    "knn.fit(data)\n",
    "\n",
    "# Calculate distances and indices of neighbors\n",
    "distances, indices = knn.kneighbors(data)\n",
    "\n",
    "# Print distances to the 5 nearest neighbors for each point\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7491503-0a74-4743-be03-5d0926a42080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f05c11a-c6e3-4952-a566-ce9735ceeb4b",
   "metadata": {},
   "source": [
    "#Q6\n",
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the local density deviation of a data point compared to its neighbors. Example using LOF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d3c2ff1-971a-41b8-8239-fd2557e63620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1 -1 -1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1\n",
      "  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.random.normal(loc=0, scale=1, size=(1000, 2))\n",
    "\n",
    "# Fit a LOF model\n",
    "lof = LocalOutlierFactor(contamination=0.05)  # 5% contamination (expected proportion of anomalies)\n",
    "scores = lof.fit_predict(data)\n",
    "\n",
    "# Print anomaly scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987e7e8-7243-4914-bbe8-86a5ff1a361c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55891aff-ce04-402d-ac22-b5f28f34100c",
   "metadata": {},
   "source": [
    "#Q7\n",
    "Key parameters of the Isolation Forest algorithm include the number of trees and the contamination parameter. Example using Isolation Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4026849a-3811-4887-ad15-7fe26951fd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1 -1  1  1  1  1\n",
      "  1  1 -1  1  1 -1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1\n",
      "  1 -1  1  1 -1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1 -1  1  1  1 -1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1 -1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1 -1  1  1 -1  1 -1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1 -1  1  1 -1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1 -1  1 -1  1  1  1  1\n",
      "  1  1  1  1  1 -1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1 -1  1 -1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1  1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
      "  1 -1  1  1  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1  1 -1  1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1 -1  1 -1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1 -1  1 -1  1  1 -1  1  1\n",
      " -1  1  1  1  1  1  1  1  1 -1  1  1 -1 -1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1 -1  1 -1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.random.normal(loc=0, scale=1, size=(1000, 2))\n",
    "\n",
    "# Fit an Isolation Forest model with 50 trees and 10% contamination\n",
    "model = IsolationForest(n_estimators=50, contamination=0.1)\n",
    "predictions = model.fit_predict(data)\n",
    "\n",
    "# Print predicted labels\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b54731-d39f-4424-bc79-438bd6016741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a05f3f-90cc-46c5-9fb6-930dcec685cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  8  8  8  8  8  8  8  8  8  8  8  3  8  8  8  8  8  1  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  1  8  8  8  8  8  8  8  8  6  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8 -1  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  2  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  3  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  1  8  8  8  8  8  8  8  8  7  8  8  8\n",
      "  8  8  8  8  8  0  5  0  8  8  7  8  8  8  8  8  8  8  8  8  8  8  7  8\n",
      "  8  8  8  4  8  0  8  8  8  8  8  8  8  8  8  8  8  8  8  8  0  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  0  8  8  8  0\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  7  8  8  8  8\n",
      "  8  8  8  8  8  8  8  5  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  6\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  6  8\n",
      "  8  8  8  8  8  8  8  8  4  8  8  8  8  8  8  8  8  8  5  6  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  0  3  8  8  8  8  8\n",
      "  8  8  8  8  8  5  0  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  1  8  8  8  8  8  8  8  8  8  7  8  8  8  8  8  8  8  4  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  0  8  8  8  8  8  8  8  8  8  8  8  8  0\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  6  8  7  8  8  8  5  8\n",
      "  0  8  8  8  8  8  8  1  8  8  8  8  8  8  8  8  8  1  8  0  8  8  8  8\n",
      "  8  8  8  8  6  8  8  8  8  8  8  8  8  8  8  8  3  8  8  8  8  8  8  4\n",
      "  8  8  8  8  8  8  8  8  8  8  7  8  8  3  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  6  8\n",
      "  8  8  8  7  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  1  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  3  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      " -1  8  8  8  8  8  8  8  8  8  8  8  8  4  3  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  5  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8 -1  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  0  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  3  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  1  1  8  8  8  8  8  8  8  8  8  8  8  8  8  8  4  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  6  0  8  8  8  8  8  8  8  8  8  8  8  3  8\n",
      "  8  8  8  8  8  8  8  8  4  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8]\n"
     ]
    }
   ],
   "source": [
    "#Q8\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.random.normal(loc=0, scale=1, size=(1000, 2))\n",
    "\n",
    "# Fit a KNN model with K=10\n",
    "knn = NearestNeighbors(n_neighbors=10)\n",
    "knn.fit(data)\n",
    "\n",
    "# Calculate distances and indices of neighbors\n",
    "distances, indices = knn.kneighbors(data)\n",
    "\n",
    "# Compute anomaly scores based on the number of neighbors within a radius of 0.5\n",
    "anomaly_scores = np.sum(distances < 0.5, axis=1) - 2  # Subtract 2 to exclude the point itself and one neighbor\n",
    "print(anomaly_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ca222-4165-4828-ac12-1c54bf8417bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd35e0d-b30d-45f5-b334-95f67036fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly score for data point 2: 0.6315866598955324\n"
     ]
    }
   ],
   "source": [
    "#Q9\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.random.normal(loc=0, scale=1, size=(3000, 2))\n",
    "\n",
    "# Fit an Isolation Forest model with 100 trees\n",
    "model = IsolationForest(n_estimators=100)\n",
    "model.fit(data)\n",
    "\n",
    "# Calculate anomaly scores based on the average path length\n",
    "average_path_length = model.decision_function(data)\n",
    "anomaly_score = 2 ** (-average_path_length / np.mean(average_path_length))  # Divide by the mean of path lengths\n",
    "\n",
    "# Print the anomaly score for a data point with an average path length of 5.0\n",
    "sample_data_point_index = 2\n",
    "print(f\"Anomaly score for data point {sample_data_point_index}: {anomaly_score[sample_data_point_index]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
